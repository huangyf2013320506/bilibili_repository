# Understanding Self-Attention in Transformers

This document provides a supplementary explanation of self-attention, inspired by the explanation around **46:50** in the following video:

ğŸ“º [Bilibili Video Link](https://www.bilibili.com/video/BV1xoJwzDESD/?spm_id_from=333.337.search-card.all.click&vd_source=594f8fd1e28e6148964bda737696c684)

---

## ğŸ” Key Differences: Self-Attention vs Traditional Attention

- **Traditional Attention** (e.g., in translation tasks):
  - Driven by the **output**, attending to the **input**.
  - Example:
    - Input: `"æˆ‘çˆ±æ°´è¯¾"`
    - Output: `"I love easy courses"`
    - Each word in the output sequence determines which parts of the input sequence to attend to, forming an **output-oriented attention** mechanism.

- **Self-Attention**:
  - Words within the **same input sequence attend to each other**.
  - This mechanism captures intra-sentence dependencies by letting every token relate to every other token.
  - Result: A **semantically enriched input matrix** using Q (Query), K (Key), and V (Value).
  - This understanding is mainly applied on the **Encoder side**, where self-attention helps to model internal relationships.

---

## ğŸ”„ Decoder-Side: Masked Self-Attention + Cross-Attention

### Input Format with Teacher Forcing

During training, we use the **teacher forcing** strategy where the decoder receives the ground truth output shifted to the right, with a `<start>` token prepended.

For example, the target sentence `"I love easy courses"` is transformed into:

`<start> I love easy courses`

The full training target becomes:

`["I", "love", "easy", "courses", "<end>"]`

- We **prepend** `<start>` to the decoder input.
- We **remove** `<end>` from the training input sequence (it remains in the target output).

### Masked Self-Attention

- Q and K are generated from this decoder input, resulting in a `[5 Ã— 512]` matrix.
- To ensure causality (i.e., no looking ahead), we apply **lower triangular masking**:
  - The upper triangular portion of the attention matrix is filled with `-inf`, so after softmax, it becomes zero.
- The resulting masked attention matrix of shape `[5 Ã— 512]` is passed on.

---

## ğŸ¯ Cross-Attention in the Decoder

- After masked self-attention, the next step is **cross-attention**, which resembles traditional attention.
- Here, the decoder attends to the encoder outputs to enrich its intermediate state.
- Unlike traditional models (often **single-head**), Transformer uses **multi-head cross-attention**.

---

## ğŸ§® Computation: Cross-Attention Step-by-Step

### Encoder Output

Suppose the encoder processes the input:

`["æˆ‘", "çˆ±", "æ°´", "è¯¾"]`
 
Resulting in a matrix of shape: `[4 Ã— 512]`

### Decoder State (from Masked Self-Attention)

For the decoder input:

`<start>, I, love, easy, courses`

The masked self-attention produces a state matrix of: `[5 Ã— 512]`

Each row corresponds to one decoding step, progressively revealing more context:

| Row | Target Prediction | Attended Tokens |
|-----|-------------------|------------------|
| 1   | I                 | `<start>` |
| 2   | love              | `<start>`, I |
| 3   | easy              | `<start>`, I, love |
| 4   | courses           | `<start>`, I, love, easy |
| 5   | `<end>`           | `<start>`, I, love, easy, courses |

---

### Step 1: Generate Q, K, V

```text
Q = Decoder state Ã— W_Q     = [5 Ã— 512] Ã— [512 Ã— 512] â†’ [5 Ã— 512]
K = Encoder output Ã— W_K    = [4 Ã— 512] Ã— [512 Ã— 512] â†’ [4 Ã— 512]
V = Encoder output Ã— W_V    = [4 Ã— 512] Ã— [512 Ã— 512] â†’ [4 Ã— 512]

Scores    = Q Ã— K^T          = [5 Ã— 512] Ã— [512 Ã— 4] â†’ [5 Ã— 4]
Weights   = softmax(scores) = [5 Ã— 4]
Output    = Weights Ã— V      = [5 Ã— 4] Ã— [4 Ã— 512] â†’ [5 Ã— 512]
```

The result is a `[5 Ã— 512]` matrix that is then passed into the next feed-forward layers.

---

## â“ Why No `<end>` in Encoder Input?

- **Encoder**: Processes input **in parallel**, inherently knows boundaries.
- **Decoder**: Generates output **step-by-step**, requires:
  - `<start>` to begin generation.
  - `<end>` to terminate generation.

---

## ğŸ™ Final Notes

This document represents my **personal understanding** of the self-attention and decoder attention mechanisms in Transformers.

If there are any mistakes or inaccuracies, your **corrections are highly appreciated**. Thank you!

# ä¸­æ–‡ç‰ˆæœ¬ Chinese Version:
# Transformer æ³¨æ„åŠ›æœºåˆ¶ä¸ªäººç†è§£è¡¥å……

æœ¬é¡¹ç›®æ˜¯å¯¹Bilibiliè§†é¢‘ [BV1xoJwzDESD](https://www.bilibili.com/video/BV1xoJwzDESD/?spm_id_from=333.337.search-card.all.click&vd_source=594f8fd1e28e6148964bda737696c684) ä¸­å¤§çº¦ 46:50 å¤„å…³äºè‡ªæ³¨æ„åŠ›æœºåˆ¶è§£é‡Šçš„ä¸ªäººç†è§£è¡¥å……ã€‚

---

## è‡ªæ³¨æ„åŠ›ä¸ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶çš„åŒºåˆ«

æˆ‘ä¸ªäººç†è§£ï¼Œ**è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰**ä¸ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ä¸»è¦åŒºåˆ«åœ¨äºå…¶å…³æ³¨çš„ç„¦ç‚¹ã€‚

* **ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶**ï¼šä»¥ç¿»è¯‘ä»»åŠ¡ä¸ºä¾‹ï¼ˆå¦‚â€œæˆ‘çˆ±æ°´è¯¾â€ç¿»è¯‘æˆ"I love easy courses"ï¼‰ï¼Œä¼ ç»Ÿæ³¨æ„åŠ›è¡¨ç°ä¸º**è¾“å‡ºå¦‚ä½•å…³æ³¨è¾“å…¥**ã€‚å®ƒæ˜¯ç”±è¾“å‡ºé©±åŠ¨çš„ï¼Œå†³å®šä»è¾“å…¥ä¸­æå–ä»€ä¹ˆä¿¡æ¯ã€‚å¯ä»¥ç†è§£ä¸ºï¼š**è¾“å‡ºé¢å‘è¾“å…¥çš„å…³æ³¨**ã€‚

* **è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šå®ƒ**åªå…³æ³¨è‡ªèº«å†…éƒ¨**ã€‚åœ¨Encoderéƒ¨åˆ†ï¼Œè‡ªæ³¨æ„åŠ›æ˜¯è¾“å…¥åºåˆ—å†…éƒ¨è¯ä¸è¯ä¹‹é—´çš„å…³ç³»å…³æ³¨ã€‚å®ƒé€šè¿‡å†…éƒ¨è¯æ±‡é—´çš„ç›¸äº’è§£é‡Šå’Œä¸°å¯Œï¼Œé€šè¿‡QKVçŸ©é˜µå¾—åˆ°ä¸€ä¸ªè¯­ä¹‰æ›´ä¸°å¯Œçš„è¾“å…¥çŸ©é˜µã€‚å¯ä»¥ç†è§£ä¸ºï¼š**è¾“å…¥å†…éƒ¨äº’ç›¸è§£é‡Šï¼Œä¸°å¯Œå½¼æ­¤çš„è¯­ä¹‰**ã€‚

    * **Encoderä¾§çš„è‡ªæ³¨æ„åŠ›**ï¼šè¾“å…¥åºåˆ—çš„æ¯ä¸ªè¯éƒ½ä¼šä¸å…¶ä»–è¯è®¡ç®—æ³¨æ„åŠ›ï¼Œä»è€Œæ•è·åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»ã€‚

---

## Decoder ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶

åœ¨Decoderéƒ¨åˆ†ï¼Œå°¤å…¶æ˜¯åœ¨ `teacher forcing` çš„è®­ç»ƒç­–ç•¥ä¸‹ï¼Œç†è§£ä¼šæœ‰äº›ä¸åŒï¼š

### Masked Self-Attention (æ©ç è‡ªæ³¨æ„åŠ›)

åœ¨Decoderçš„ç¬¬ä¸€å±‚ï¼ˆMasked Self-Attentionï¼‰ï¼Œè¾“å…¥çš„æ–‡æœ¬æ˜¯ `"<start> I love easy courses"`ï¼ˆè¿™æ˜¯é¢„æœŸè¾“å‡ºæ–‡æœ¬æ•´ä½“å³ç§»å¹¶åœ¨å¼€å¤´æ·»åŠ  `<start>` ä¿¡å·ï¼Œå…¶åŸæœ¬å®Œæ•´çš„è®­ç»ƒç›®æ ‡æ˜¯ `["I", "love", "easy", "courses", "<end>"]`ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬ç§»é™¤äº†å³ä¾§çš„ `<end>` å¹¶åœ¨å·¦ä¾§æ·»åŠ äº† `<start>`ï¼‰ã€‚

è¿›è¡ŒQã€Kä¸¤ä¸ªçŸ©é˜µçš„çº¿æ€§å˜æ¢åä¼šå¾—åˆ°ä¸€ä¸ª $5 \times 5$ çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œç„¶åä¼šè¿›è¡Œ**ä¸‹ä¸‰è§’æ©ç **æ“ä½œï¼ˆå°†ä¸Šä¸‰è§’æ³¨æ„åŠ›è®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§ï¼Œè¿™æ ·ç»è¿‡ `softmax` åé‚£éƒ¨åˆ†å°±ä¼šå˜æˆ $0$ï¼‰ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ª $5 \times 512$ ä¸”ç»è¿‡â€œå¸¦æ©ç çš„æ³¨æ„åŠ›çŸ©é˜µâ€å¤„ç†è¿‡åçš„çŸ©é˜µã€‚è¿™ä¸ªçŸ©é˜µå°†ä½œä¸ºè¾“å…¥ä¼ é€’åˆ°ä¸‹ä¸€ä¸ª **Multi-head Attention** æ¨¡å—ã€‚

### Cross-Attention (äº¤å‰æ³¨æ„åŠ›)

Decoderä¸­çš„ç¬¬äºŒä¸ªå¤šå¤´æ³¨æ„åŠ›æ¨¡å—ä¸å†æ˜¯Encoderä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œè€Œæ˜¯**äº¤å‰æ³¨æ„åŠ›æ¨¡å—**ã€‚å®ƒæ›´ç±»ä¼¼äºä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå°½ç®¡åŒºåˆ«åœ¨äºTransformerçš„è§£ç å™¨ä¸­ä½¿ç”¨çš„æ˜¯å¤šå¤´çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè€Œä¼ ç»Ÿçš„äº¤å‰æ³¨æ„åŠ›å¤§å¤šæ•°æ˜¯å•å¤´ï¼‰ï¼Œå®ƒæ³¨é‡**Decoderçš„å½“å‰çŠ¶æ€**ï¼ˆ`"I love easy courses"`ï¼‰å’Œ**Encoderçš„è¾“å‡º**ï¼ˆ`â€œæˆ‘çˆ±æ°´è¯¾â€`ï¼‰ä¹‹é—´çš„å…³ç³»ã€‚

#### äº¤å‰æ³¨æ„åŠ›è®¡ç®—æµç¨‹ä¸ªäººç†è§£ï¼š

* **Encoderè¾“å‡º**ï¼š $[4 \times 512]$ ï¼ˆè¡¨ç¤º`["æˆ‘","çˆ±","æ°´","è¯¾"]`çš„ç¼–ç ï¼‰
* **DecoderçŠ¶æ€**ï¼š $[5 \times 512]$ ï¼ˆæ¥è‡ª Masked Self-Attention çš„è¾“å‡ºï¼‰
    è¿™é‡Œçš„DecoderçŠ¶æ€å®é™…ä¸Šæ˜¯äº”è¡Œé’ˆå¯¹ä¸åŒæ©ç æƒ…å†µæ—¶ç”Ÿæˆçš„ $512$ ç»´çš„è¡Œå‘é‡ï¼š
    * ç¬¬ä¸€è¡Œï¼šåªå…³æ³¨ `"<start>"`ï¼Œç”¨äºé¢„æµ‹ `"I"`ã€‚
    * ç¬¬äºŒè¡Œï¼šå…³æ³¨ `"<start>"` å’Œ `"I"`ï¼Œç”¨äºé¢„æµ‹ `"love"`ã€‚
    * ç¬¬ä¸‰è¡Œï¼šå…³æ³¨ `"<start>"`, `"I"`, `"love"`ï¼Œç”¨äºé¢„æµ‹ `"easy"`ã€‚
    * ç¬¬å››è¡Œï¼šå…³æ³¨ `"<start>"`, `"I"`, `"love"`, `"easy"`ï¼Œç”¨äºé¢„æµ‹ `"courses"`ã€‚
    * ç¬¬äº”è¡Œï¼šå…³æ³¨ `"<start>"`, `"I"`, `"love"`, `"easy"`, `"courses"`ï¼Œç”¨äºé¢„æµ‹ `"<end>"`ã€‚

**æ­¥éª¤ 1ï¼šç”Ÿæˆ Q, K, V çŸ©é˜µ**

* $Q = \text{DecoderçŠ¶æ€} \times W_Q = [5 \times 512] \times [512 \times 512] = [5 \times 512]$
* $K = \text{Encoderè¾“å‡º} \times W_K = [4 \times 512] \times [512 \times 512] = [4 \times 512]$
* $V = \text{Encoderè¾“å‡º} \times W_V = [4 \times 512] \times [512 \times 512] = [4 \times 512]$

**æ­¥éª¤ 2ï¼šè®¡ç®—æ³¨æ„åŠ›**

* $\text{æ³¨æ„åŠ›åˆ†æ•°} = Q \times K^T = [5 \times 512] \times [512 \times 4] = [5 \times 4]$
* $\text{æ³¨æ„åŠ›æƒé‡} = \text{softmax}(\text{æ³¨æ„åŠ›åˆ†æ•°}) = [5 \times 4]$
* $\text{è¾“å‡º} = \text{æ³¨æ„åŠ›æƒé‡} \times V = [5 \times 4] \times [4 \times 512] = [5 \times 512]$

---

## ä¸ºä»€ä¹ˆEncoderä¾§çš„è¾“å…¥ä¸éœ€è¦ `<end>` æ ‡è®°ï¼Ÿ

* **Encoder**ï¼šå®ƒå¹¶è¡Œå¤„ç†æ•´ä¸ªè¾“å…¥åºåˆ—ï¼Œå¤©ç„¶åœ°çŸ¥é“åºåˆ—çš„è¾¹ç•Œã€‚
* **Decoder**ï¼šå®ƒé¢å‘è¾“å‡ºé€æ­¥ç”Ÿæˆï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªæ˜ç¡®çš„åœæ­¢ä¿¡å· (`<end>`) æ¥æŒ‡ç¤ºä½•æ—¶åœæ­¢ç”Ÿæˆã€‚

---

**ä»¥ä¸Šæ˜¯æˆ‘ä¸ªäººçš„ç†è§£ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œè¿˜æœ›å¤§å®¶æ–§æ­£ï¼Œéå¸¸æ„Ÿè°¢ï¼**
